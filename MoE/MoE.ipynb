{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## import"
      ],
      "metadata": {
        "id": "fSwJCJL7wlgo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YskPcd34weLM",
        "outputId": "c6acab97-75a9-4aa1-ffb4-c528a5a1ff28"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8364a20f50>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as f\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load The Data"
      ],
      "metadata": {
        "id": "6nD9U70HzPqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/input.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3ILwwMMxAr-",
        "outputId": "b68b8e94-1ba6-4046-d254-26c01ef099d8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-20 04:37:55--  https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-10-20 04:37:55 (31.6 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Experts"
      ],
      "metadata": {
        "id": "BkhN60WYzM-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Expert(nn.Module):\n",
        "  def __init__(self,n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(4 * n_embd, n_embd),\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "n2_-36k5xQ3K"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Impliment The Router"
      ],
      "metadata": {
        "id": "VW6MuxdEzDeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_expert=4\n",
        "top_k=3\n",
        "n_embed = 32\n",
        "\n",
        "\n",
        "# Example\n",
        "mh_output = torch.rand(2,4,n_embed)\n",
        "topkgate_linear = nn.Linear(n_embed, num_expert)  # 32 x 4\n",
        "logist = topkgate_linear(mh_output)\n",
        "\n",
        "print(logist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0FkMto5y_qG",
        "outputId": "0b1cfd8d-c5ee-4dd3-bbd5-cc037ce0dec3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.2417, -0.1422, -0.8395,  0.0696],\n",
            "         [-0.0817, -0.0317, -0.8352, -0.0116],\n",
            "         [-0.0519,  0.2970, -0.9370, -0.1549],\n",
            "         [-0.0545, -0.0121, -0.6304, -0.1077]],\n",
            "\n",
            "        [[-0.0450,  0.3167, -0.3820,  0.0872],\n",
            "         [-0.0316,  0.1769, -1.0049,  0.2885],\n",
            "         [ 0.0063,  0.3378, -0.6606,  0.1216],\n",
            "         [-0.0663, -0.0898, -0.5614, -0.1854]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Balancing"
      ],
      "metadata": {
        "id": "bWcfXnDc0kHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topk_logist, topk_indices = logist.topk(top_k,dim=1)\n",
        "topk_logist, topk_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvzqeZcE0Rno",
        "outputId": "687aa361-fdfb-4f94-d776-9fbe0f030359"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[-0.0519,  0.2970, -0.6304,  0.0696],\n",
              "          [-0.0545, -0.0121, -0.8352, -0.0116],\n",
              "          [-0.0817, -0.0317, -0.8395, -0.1077]],\n",
              " \n",
              "         [[ 0.0063,  0.3378, -0.3820,  0.2885],\n",
              "          [-0.0316,  0.3167, -0.5614,  0.1216],\n",
              "          [-0.0450,  0.1769, -0.6606,  0.0872]]], grad_fn=<TopkBackward0>),\n",
              " tensor([[[2, 2, 3, 0],\n",
              "          [3, 3, 1, 1],\n",
              "          [1, 1, 0, 3]],\n",
              " \n",
              "         [[2, 2, 0, 1],\n",
              "          [1, 0, 3, 2],\n",
              "          [0, 1, 2, 0]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## -infinity And Apply Softmax"
      ],
      "metadata": {
        "id": "MSLU4DB71Gmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import inf\n",
        "zeros = torch.full_like(logist,float('-inf'))\n",
        "sparse_logist = zeros.scatter(-1, topk_indices, topk_logist)\n",
        "sparse_logist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01Lx2x1G03nR",
        "outputId": "4215a5b7-3fc9-40f4-9dbf-8e04d46627e7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0696,    -inf,  0.2970, -0.6304],\n",
              "         [   -inf, -0.0116,    -inf, -0.0121],\n",
              "         [-0.8395, -0.0317,    -inf, -0.1077],\n",
              "         [   -inf,    -inf,    -inf,    -inf]],\n",
              "\n",
              "        [[-0.3820,  0.2885,  0.3378,    -inf],\n",
              "         [ 0.3167, -0.0316,  0.1216, -0.5614],\n",
              "         [ 0.0872,  0.1769, -0.6606,    -inf],\n",
              "         [   -inf,    -inf,    -inf,    -inf]]], grad_fn=<ScatterBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inplace of inf we are putting zeros\n",
        "getting_output = f.softmax(sparse_logist, dim=1)\n",
        "getting_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yodZYnkw2Slk",
        "outputId": "1c86cfdb-2df3-42a1-bb78-6e8b2669a253"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.7128, 0.0000, 1.0000, 0.2201],\n",
              "         [0.0000, 0.5050, 0.0000, 0.4086],\n",
              "         [0.2872, 0.4950, 0.0000, 0.3713],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
              "\n",
              "        [[0.2169, 0.3816, 0.4600, 0.0000],\n",
              "         [0.4363, 0.2771, 0.3706, 1.0000],\n",
              "         [0.3468, 0.3413, 0.1695, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class for Topk Routing"
      ],
      "metadata": {
        "id": "OdynQ58b20oB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First define the top k router module\n",
        "class TopkRouter(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k):\n",
        "        super(TopkRouter, self).__init__()\n",
        "        self.top_k = top_k\n",
        "        self.linear = nn.Linear(n_embed, num_experts)\n",
        "\n",
        "    def forward(self, mh_output):\n",
        "        # mh_output is the output tensor from multihead self-attention block\n",
        "        logits = self.linear(mh_output)\n",
        "        top_k_logits, indices = logits.topk(self.top_k, dim=-1)\n",
        "        zeros = torch.full_like(logits, float('-inf'))\n",
        "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
        "        router_output = f.softmax(sparse_logits, dim=-1)\n",
        "        return router_output, indices\n"
      ],
      "metadata": {
        "id": "popTpP0H2r8b"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_experts=3\n",
        "top_k=2\n",
        "n_embd=8\n",
        "\n",
        "# Example\n",
        "mh_output = torch.rand(1,4,n_embed)\n",
        "top_k_gate =TopkRouter(n_embed, num_expert, top_k)\n",
        "getting_output, indices = top_k_gate(mh_output)\n",
        "\n",
        "getting_output.shape, getting_output, indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gF-p-bq3nPp",
        "outputId": "257d4de5-aced-48c0-be07-f83d1bbdf1e3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 4, 4]),\n",
              " tensor([[[0.0000, 0.5049, 0.4951, 0.0000],\n",
              "          [0.4959, 0.0000, 0.5041, 0.0000],\n",
              "          [0.6062, 0.0000, 0.3938, 0.0000],\n",
              "          [0.5327, 0.0000, 0.4673, 0.0000]]], grad_fn=<SoftmaxBackward0>),\n",
              " tensor([[[1, 2],\n",
              "          [2, 0],\n",
              "          [0, 2],\n",
              "          [0, 2]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Noisy Top K"
      ],
      "metadata": {
        "id": "Dtcc-Y1O40CH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NoisyTopkRouter(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k):\n",
        "        super(NoisyTopkRouter, self).__init__()\n",
        "        self.top_k = top_k\n",
        "\n",
        "        # layer for router logits\n",
        "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
        "        self.noise_linear = nn.Linear(n_embed, num_experts)\n",
        "\n",
        "    def forward(self, mh_output):\n",
        "        # mh_output is the output tensor from multihead self attention block\n",
        "        logits = self.topkroute_linear(mh_output)\n",
        "\n",
        "        # Noise logits\n",
        "        noise_logits = self.noise_linear(mh_output)\n",
        "\n",
        "        # Adding scaled unit Gaussian noise to the logits\n",
        "        noise = torch.randn_like(logits) * f.softplus(noise_logits)\n",
        "        noisy_logits = logits + noise\n",
        "\n",
        "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
        "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
        "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
        "        router_output = f.softmax(sparse_logits, dim=-1)\n",
        "        return router_output, indices\n"
      ],
      "metadata": {
        "id": "5wh7faOJ4eZn"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing this out, again:\n",
        "num_experts = 3\n",
        "top_k = 2\n",
        "n_embd = 8\n",
        "\n",
        "mh_output = torch.randn(1, 4, n_embd)  # Example input\n",
        "noisy_top_k_gate = NoisyTopkRouter(n_embd, num_experts, top_k)\n",
        "gating_output, indices = noisy_top_k_gate(mh_output)\n",
        "\n",
        "gating_output.shape, gating_output, indices\n",
        "# ✅ It works!!\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmxY2NMK5HXS",
        "outputId": "2d301367-de5c-4f6f-8056-3ff20b07a6b7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 4, 3]),\n",
              " tensor([[[0.0000, 0.6529, 0.3471],\n",
              "          [0.1302, 0.0000, 0.8698],\n",
              "          [0.4382, 0.0000, 0.5618],\n",
              "          [0.5730, 0.4270, 0.0000]]], grad_fn=<SoftmaxBackward0>),\n",
              " tensor([[[1, 2],\n",
              "          [2, 0],\n",
              "          [2, 0],\n",
              "          [0, 1]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SparseMoE(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k):\n",
        "        super(SparseMoE, self).__init__()\n",
        "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
        "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def forward(self, x):\n",
        "        gating_output, indices = self.router(x)\n",
        "        final_output = torch.zeros_like(x)\n",
        "\n",
        "        # Reshape inputs for batch processing\n",
        "        flat_x = x.view(-1, x.size(-1))\n",
        "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
        "\n",
        "        # Process each expert in parallel\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            # Create a mask for the inputs where the current expert is in top-k\n",
        "            expert_mask = (indices == i).any(dim=-1)\n",
        "            flat_mask = expert_mask.view(-1)\n",
        "\n",
        "            if flat_mask.any():\n",
        "                expert_input = flat_x[flat_mask]\n",
        "                expert_output = expert(expert_input)\n",
        "\n",
        "                # Extract and apply gating scores\n",
        "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
        "                weighted_output = expert_output * gating_scores\n",
        "\n",
        "                # Update final output additively by indexing and adding\n",
        "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
        "\n",
        "        return final_output\n"
      ],
      "metadata": {
        "id": "nUckXfyW5gIT"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test this out\n",
        "num_experts = 3\n",
        "top_k = 2\n",
        "n_embd = 8\n",
        "dropout = 0.1\n",
        "\n",
        "mh_output = torch.randn(1, 4, n_embd)  # Example multi-head attention output\n",
        "sparse_moe = SparseMoE(n_embd, num_experts, top_k)\n",
        "final_output = sparse_moe(mh_output)\n",
        "\n",
        "print(\"Shape of the final output:\", final_output.shape)\n",
        "print(final_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy7jtqfC57UD",
        "outputId": "e73d8f8a-8a63-472f-b461-dd718e99a3dd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the final output: torch.Size([1, 4, 8])\n",
            "tensor([[[ 0.0376, -0.0033, -0.0288,  0.0254,  0.3026,  0.3510, -0.1571,\n",
            "          -0.2406],\n",
            "         [ 0.1961, -0.0018, -0.1411, -0.0173,  0.2917, -0.0550, -0.0507,\n",
            "          -0.0425],\n",
            "         [-0.0573, -0.0112,  0.0427, -0.0455,  0.0413,  0.1064, -0.1094,\n",
            "          -0.0904],\n",
            "         [-0.0945, -0.1488, -0.0067, -0.1033, -0.1375,  0.0000,  0.0921,\n",
            "          -0.2591]]], grad_fn=<IndexPutBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting all together"
      ],
      "metadata": {
        "id": "3QYbkHAY56Ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class NoisyTopkRouter(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k):\n",
        "        super(NoisyTopkRouter, self).__init__()\n",
        "        self.top_k = top_k\n",
        "        # layer for router logits\n",
        "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
        "        self.noise_linear = nn.Linear(n_embed, num_experts)\n",
        "\n",
        "    def forward(self, mh_output):\n",
        "        # mh_output is the output tensor from multihead self attention block\n",
        "        logits = self.topkroute_linear(mh_output)\n",
        "\n",
        "        # Noise logits\n",
        "        noise_logits = self.noise_linear(mh_output)\n",
        "\n",
        "        # Adding scaled unit Gaussian noise to the logits\n",
        "        noise = torch.randn_like(logits) * F.softplus(noise_logits)\n",
        "        noisy_logits = logits + noise\n",
        "\n",
        "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
        "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
        "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
        "        router_output = F.softmax(sparse_logits, dim=-1)\n",
        "        return router_output, indices\n",
        "\n",
        "\n",
        "\n",
        "class Expert(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super(Expert, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_embed, n_embed)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class SparseMoE(nn.Module):\n",
        "    def __init__(self, n_embed, num_experts, top_k):\n",
        "        super(SparseMoE, self).__init__()\n",
        "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
        "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def forward(self, x):\n",
        "        gating_output, indices = self.router(x)\n",
        "        final_output = torch.zeros_like(x)\n",
        "\n",
        "        # Reshape inputs for batch processing\n",
        "        flat_x = x.view(-1, x.size(-1))\n",
        "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
        "\n",
        "        # Process each expert in parallel\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            # Create a mask for the inputs where the current expert is in top-k\n",
        "            expert_mask = (indices == i).any(dim=-1)\n",
        "            flat_mask = expert_mask.view(-1)\n",
        "\n",
        "            if flat_mask.any():\n",
        "                expert_input = flat_x[flat_mask]\n",
        "                expert_output = expert(expert_input)\n",
        "\n",
        "                # Extract and apply gating scores\n",
        "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
        "                weighted_output = expert_output * gating_scores\n",
        "\n",
        "                # Update final output additively by indexing and adding\n",
        "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
        "\n",
        "        return final_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE3dp-rs5oYA",
        "outputId": "aeff3ff6-ddbf-465d-88a3-b04b25319b11"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the final output: torch.Size([1, 4, 8])\n",
            "tensor([[[-0.3353,  0.1319, -0.3851,  0.2569, -0.4109, -0.3995,  0.2535,\n",
            "          -0.0176],\n",
            "         [-0.2706, -0.0220,  0.0683,  0.1375, -0.1583, -0.3143, -0.0646,\n",
            "          -0.0106],\n",
            "         [-0.1619, -0.1402, -0.3502,  0.2790, -0.0967,  0.0784, -0.2770,\n",
            "           0.2086],\n",
            "         [-0.2057, -0.0115, -0.1763,  0.3149, -0.0429, -0.1158, -0.0741,\n",
            "          -0.0524]]], grad_fn=<IndexPutBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_experts = 3\n",
        "top_k = 2\n",
        "n_embd = 8\n",
        "dropout = 0.1\n",
        "\n",
        "mh_output = torch.randn(1, 4, n_embd)  # Example multi-head attention output\n",
        "sparse_moe = SparseMoE(n_embd, num_experts, top_k)\n",
        "final_output = sparse_moe(mh_output)\n",
        "\n",
        "print(\"Shape of the final output:\", final_output.shape)\n",
        "print(final_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38qwZyB561Jc",
        "outputId": "6026af29-a838-43fb-a93e-6eaa05644427"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the final output: torch.Size([1, 4, 8])\n",
            "tensor([[[ 0.1626,  0.0525,  0.2134,  0.3373, -0.0790,  0.0268, -0.2781,\n",
            "          -0.1298],\n",
            "         [ 0.2976, -0.0684,  0.0317, -0.0590, -0.0747,  0.2387, -0.1330,\n",
            "           0.0526],\n",
            "         [ 0.0297,  0.0705,  0.0088,  0.1307, -0.0159,  0.2565, -0.1354,\n",
            "          -0.1547],\n",
            "         [ 0.2979, -0.0690, -0.0143, -0.3176, -0.4144,  0.5400,  0.0038,\n",
            "          -0.1146]]], grad_fn=<IndexPutBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)      # (B, T, C)\n",
        "        q = self.query(x)    # (B, T, C)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5      # (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)                 # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)                            # (B, T, C)\n",
        "        out = wei @ v                                # (B, T, C)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "aP9CR6eO7iz6"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Headed Self Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(n_embed, head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "FgDSm2zv7ktb"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Block"
      ],
      "metadata": {
        "id": "lqfuUtw_7bs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Mixture of Experts Transformer block: communication followed by computation (multi-head self attention) \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head, num_experts, top_k):\n",
        "        # n_embed: embedding dimension, n_head: number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_embed, n_head, head_size)\n",
        "        self.smoe = SparseMoE(n_embed, num_experts, top_k)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.smoe(self.ln2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "rLt2_gKk65dD"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Arch"
      ],
      "metadata": {
        "id": "dMSHPB_e70BR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# assume these globals are defined somewhere in your script\n",
        "# vocab_size, n_embed, block_size, n_head, n_layer, num_experts, top_k, device = ...\n",
        "# and classes: Block (uses MultiHeadAttention + SparseMoE)\n",
        "\n",
        "class SparseMoELanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table   = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[Block(n_embed, n_head, num_experts=num_experts, top_k=top_k)\n",
        "              for _ in range(n_layer)]\n",
        "        )\n",
        "        self.ln_f = nn.LayerNorm(n_embed)            # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)                         # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
        "        x = tok_emb + pos_emb                                             # (B,T,C)\n",
        "        x = self.blocks(x)                                                # (B,T,C)\n",
        "        x = self.ln_f(x)                                                  # (B,T,C)\n",
        "        logits = self.lm_head(x)                                          # (B,T,V)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Autoregressively sample next tokens.\n",
        "        idx: LongTensor of shape (B, T)\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]                 # (B, T_ctx)\n",
        "            logits, _ = self(idx_cond)                      # (B, T_ctx, V)\n",
        "            logits = logits[:, -1, :]                       # (B, V)\n",
        "            probs = F.softmax(logits, dim=-1)               # (B, V)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            idx = torch.cat([idx, idx_next], dim=1)         # (B, T+1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "N--f3vba7ziR"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainig and testing data"
      ],
      "metadata": {
        "id": "5GHYpiJC8uAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Read the dataset\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# --- Create character-level vocabulary ---\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "# Mappings (char ↔ index)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]                # string → list of ints\n",
        "decode = lambda l: ''.join([itos[i] for i in l])       # list of ints → string\n",
        "\n",
        "# --- Train / Validation Split ---\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))  # first 90% train, rest validation\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# --- Dataloader Function ---\n",
        "def get_batch(split):\n",
        "    \"\"\"Generate a small batch of data for inputs (x) and targets (y).\"\"\"\n",
        "    data_split = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
        "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrSGWaa98sa6",
        "outputId": "474db61a-9dc7-40fb-cdce-8c9694cb1524"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define LLM loss"
      ],
      "metadata": {
        "id": "1qI-cQ0B9Hmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "Cd0oEvrL9HBL"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop pararms and hyper params"
      ],
      "metadata": {
        "id": "UVw10Dkw9Y4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 14: Define training loop parameters and other hyperparameters\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "\n",
        "# ----------------\n",
        "# Hyperparameters\n",
        "# ----------------\n",
        "batch_size = 16          # how many independent sequences will we process in parallel?\n",
        "block_size = 32          # what is the maximum context length for predictions?\n",
        "max_iters = 200          # total training iterations  increase this if you want accurate result 60k 100k\n",
        "eval_interval = 100      # evaluate the model every N steps\n",
        "learning_rate = 1e-3     # optimizer learning rate\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 400         # number of iterations for evaluation\n",
        "head_size = 16\n",
        "n_embed = 128\n",
        "n_head = 8\n",
        "n_layer = 8\n",
        "dropout = 0.1\n",
        "num_experts = 8\n",
        "top_k = 2\n"
      ],
      "metadata": {
        "id": "pfgzAoct9Wfv"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Init Model"
      ],
      "metadata": {
        "id": "qT8-4tQV9lWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kaiming_init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "model = SparseMoELanguageModel()\n",
        "model.apply(kaiming_init_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbQt1sEb9nIT",
        "outputId": "01dcb50c-16df-49e1-9e5d-6be29f358bac"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SparseMoELanguageModel(\n",
              "  (token_embedding_table): Embedding(65, 128)\n",
              "  (position_embedding_table): Embedding(32, 128)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopkRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopkRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopkRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopkRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (4): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopkRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (5): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopkRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (6): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopkRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (7): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopkRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=128, out_features=65, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Loop"
      ],
      "metadata": {
        "id": "BYcsJj_3-BsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 16: Run the pre-training loop\n",
        "\n",
        "# move model to device\n",
        "m = model.to(device)\n",
        "\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXjXeD1f97lX",
        "outputId": "7a3cf4dd-ea7c-4d4c-e57c-311b8de667ff"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.680513 M parameters\n",
            "step 0: train loss 5.2074, val loss 5.2143\n",
            "step 100: train loss 2.7442, val loss 2.7565\n",
            "step 199: train loss 2.5168, val loss 2.5133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infrance\n"
      ],
      "metadata": {
        "id": "6Xr-Uk_6-QsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 17: Inference\n",
        "\n",
        "# generate from the model. Not great. Not too bad either\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVOn42m3-JBZ",
        "outputId": "86d003e1-6e63-450f-bcaa-b411aea49b07"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " tanth moreve thst uredazint I heer or amanee sts the\n",
            "Or shtilord t.\n",
            "ano hee:\n",
            "Se herxher porste thenk shore pan ar wegerece.\n",
            "\n",
            "Ther hamy tis,\n",
            "CAGENAwdov:\n",
            "DETUNERE:\n",
            "WMIOur  s ham:\n",
            "IGpUC:\n",
            "DN:\n",
            "Fot slan nkio y sthereixe t durst-parcattluut,\n",
            "Anes mevil musers torsheromanor mes ghy t;:\n",
            "Areotow gores ye fovisu r, hae ssirghof.\n",
            "\n",
            "Sowes st thesthafipree w :\n",
            "We fhe thure myoofond y fuavat y:\n",
            "A:\n",
            "fout y, theyt haaC, un theutiny thereinof uren ge t thanoour pu oumee.\n",
            "Fevey re:\n",
            "Wanoy wzuavere hiceng ard prseroof te Eru lmt pu mupe men lanerenous fopr hane wsterser h menof're zhan\n",
            "IN sino mer walsthas t Githe str;\n",
            "\n",
            "ITiUForur ms t BowofEseep wink avesow atr f te, lle inin wnger,\n",
            "Pmincos thavanksu tbeant totCred,\n",
            "\n",
            "By as A:\n",
            "OAn wureearyode allancoyo bory th,\n",
            "Aser torene'n, tupusrson manven n oraknd wis m'earar chaus y'd-han t wown be, here sourd,\n",
            "Whocororfaw my phee, erese prire,\n",
            "Therandinch'le bente ay ow thin isf h manan kst setof it m:\n",
            "Clisowecir ther ntheeut tstherouff t t kind wad\n",
            "Bu bur s ay wo HureMutie; skballe ak oske, t.\n",
            "\n",
            "\n",
            "GAURAAnthofy tPNuMur?IAAR: ur Gose:\n",
            "Sufophet t hame wa ste t illthey panth k st ghetur is I sd thir Garashe ofuthin,\n",
            "Lacor t, ise'esureno thelllemangut meis l whey;\n",
            "Se? woore ad thar n t:\n",
            "MANod haosit hipre hes whes br mus me:\n",
            "Ahes towyout I haseam ssexe d t me fealang, thyour's tod we,\n",
            "Whay : icerst semu wthirosur he skic sicorapre kss thace\n",
            "Wo qRAmous highe thenqexunt t f t thininous,\n",
            "Ano yotoy'ser reve s ncodpo, ! sotd, fot y nor n, or ll avee.\n",
            "Tnor sr vy to schyofr f torcuf dxervemuthe y,\n",
            "Bathat me thinet thererehepo brukurs yry he thacorsewisepontispe s thy.\n",
            "Thithurothest aver thoung ghe m.\n",
            "\n",
            "AROCFus O'CA:\n",
            "H'NONCNATWWhind a'lit prothimen fure p orare ther this'st uses stsowithe theafers\n",
            "\n",
            "Why thrink alat thin Goo y heaud woor sth memy pake Inonor,\n",
            "Furome geancrebu t fadouecayo prlenanychot wy\n",
            "INB:\n",
            "Ameseererevemur houns, o sh whou womurnorf\n",
            "Futomancinord cy ks bus nf?yedlis bnan, d avest teinge, wure sth pyour're spache tonsos oresis heethy bear y me iobu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 🧪 Notes & Recommendations\n",
        "\n",
        "This project is built for **experimentation and research demonstration**.  \n",
        "If you want **higher accuracy or more stable results**, try the following:\n",
        "\n",
        "- 🔁 **Increase training iterations** — Run `max_iters` between **60K–100K** if you have a **high-end GPU (RTX 4090 / A100 / H100)**.  \n",
        "- ⚙️ **Tune hyperparameters** — Experiment with `learning_rate`, `num_experts`, `top_k`, and `n_embed` for your dataset scale.\n",
        "- 💾 **Save and share your trained weights** — Push the best-performing checkpoints to **[Hugging Face Hub](https://huggingface.co)** for community use.\n",
        "- 💻 **Deploy interactively** — Create a **Gradio** or **Streamlit** web interface to chat or generate text directly from the model.\n",
        "\n",
        "---\n",
        "\n",
        "✅ *This notebook is a complete end-to-end implementation — from dataset to inference.*  \n",
        "💡 *If you enjoyed this project or found it useful, please consider giving it an ⭐ on GitHub or an upvote on Kaggle!*\n",
        "\n",
        "**Thank you!**\n"
      ],
      "metadata": {
        "id": "lSFw5TEs_S1I"
      }
    }
  ]
}