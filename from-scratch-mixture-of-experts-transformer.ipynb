{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6141aa1",
   "metadata": {
    "id": "fSwJCJL7wlgo",
    "papermill": {
     "duration": 0.005612,
     "end_time": "2025-10-20T09:13:45.944957",
     "exception": false,
     "start_time": "2025-10-20T09:13:45.939345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d307ea1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:45.955511Z",
     "iopub.status.busy": "2025-10-20T09:13:45.955247Z",
     "iopub.status.idle": "2025-10-20T09:13:49.683346Z",
     "shell.execute_reply": "2025-10-20T09:13:49.682493Z"
    },
    "id": "YskPcd34weLM",
    "outputId": "c6acab97-75a9-4aa1-ffb4-c528a5a1ff28",
    "papermill": {
     "duration": 3.734715,
     "end_time": "2025-10-20T09:13:49.684592",
     "exception": false,
     "start_time": "2025-10-20T09:13:45.949877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x78fcdcebb230>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as f\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380bf1b",
   "metadata": {
    "id": "6nD9U70HzPqK",
    "papermill": {
     "duration": 0.00461,
     "end_time": "2025-10-20T09:13:49.694285",
     "exception": false,
     "start_time": "2025-10-20T09:13:49.689675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb3c2d9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:49.704870Z",
     "iopub.status.busy": "2025-10-20T09:13:49.704171Z",
     "iopub.status.idle": "2025-10-20T09:13:50.116606Z",
     "shell.execute_reply": "2025-10-20T09:13:50.115767Z"
    },
    "id": "Z3ILwwMMxAr-",
    "outputId": "b68b8e94-1ba6-4046-d254-26c01ef099d8",
    "papermill": {
     "duration": 0.418988,
     "end_time": "2025-10-20T09:13:50.117942",
     "exception": false,
     "start_time": "2025-10-20T09:13:49.698954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-20 09:13:49--  https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/input.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1115394 (1.1M) [text/plain]\r\n",
      "Saving to: ‘input.txt’\r\n",
      "\r\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \r\n",
      "\r\n",
      "2025-10-20 09:13:50 (22.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# we will download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/input.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c27e18",
   "metadata": {
    "id": "BkhN60WYzM-z",
    "papermill": {
     "duration": 0.004931,
     "end_time": "2025-10-20T09:13:50.128365",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.123434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47170512",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.139203Z",
     "iopub.status.busy": "2025-10-20T09:13:50.138936Z",
     "iopub.status.idle": "2025-10-20T09:13:50.144158Z",
     "shell.execute_reply": "2025-10-20T09:13:50.143546Z"
    },
    "id": "n2_-36k5xQ3K",
    "papermill": {
     "duration": 0.012043,
     "end_time": "2025-10-20T09:13:50.145200",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.133157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "  def __init__(self,n_embd):\n",
    "    super().__init__()\n",
    "    self.net = nn.Sequential(\n",
    "        nn.Linear(n_embd, 4 * n_embd),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(4 * n_embd, n_embd),\n",
    "        nn.Dropout(dropout),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a889911e",
   "metadata": {
    "id": "VW6MuxdEzDeN",
    "papermill": {
     "duration": 0.004557,
     "end_time": "2025-10-20T09:13:50.154569",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.150012",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## implement The Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39148848",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.164952Z",
     "iopub.status.busy": "2025-10-20T09:13:50.164707Z",
     "iopub.status.idle": "2025-10-20T09:13:50.276565Z",
     "shell.execute_reply": "2025-10-20T09:13:50.275663Z"
    },
    "id": "B0FkMto5y_qG",
    "outputId": "0b1cfd8d-c5ee-4dd3-bbd5-cc037ce0dec3",
    "papermill": {
     "duration": 0.118398,
     "end_time": "2025-10-20T09:13:50.277703",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.159305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2417, -0.1422, -0.8395,  0.0696],\n",
      "         [-0.0817, -0.0317, -0.8352, -0.0116],\n",
      "         [-0.0519,  0.2970, -0.9370, -0.1549],\n",
      "         [-0.0545, -0.0121, -0.6304, -0.1077]],\n",
      "\n",
      "        [[-0.0450,  0.3167, -0.3820,  0.0872],\n",
      "         [-0.0316,  0.1769, -1.0049,  0.2885],\n",
      "         [ 0.0063,  0.3378, -0.6606,  0.1216],\n",
      "         [-0.0663, -0.0898, -0.5614, -0.1854]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "num_expert=4\n",
    "top_k=3\n",
    "n_embed = 32\n",
    "\n",
    "\n",
    "# Example\n",
    "mh_output = torch.rand(2,4,n_embed)\n",
    "topkgate_linear = nn.Linear(n_embed, num_expert)  # 32 x 4\n",
    "logist = topkgate_linear(mh_output)\n",
    "\n",
    "print(logist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5940e7e",
   "metadata": {
    "id": "bWcfXnDc0kHN",
    "papermill": {
     "duration": 0.005081,
     "end_time": "2025-10-20T09:13:50.288069",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.282988",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b42f8a77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.298529Z",
     "iopub.status.busy": "2025-10-20T09:13:50.298309Z",
     "iopub.status.idle": "2025-10-20T09:13:50.308713Z",
     "shell.execute_reply": "2025-10-20T09:13:50.308023Z"
    },
    "id": "BvzqeZcE0Rno",
    "outputId": "687aa361-fdfb-4f94-d776-9fbe0f030359",
    "papermill": {
     "duration": 0.016833,
     "end_time": "2025-10-20T09:13:50.309717",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.292884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0519,  0.2970, -0.6304,  0.0696],\n",
       "          [-0.0545, -0.0121, -0.8352, -0.0116],\n",
       "          [-0.0817, -0.0317, -0.8395, -0.1077]],\n",
       " \n",
       "         [[ 0.0063,  0.3378, -0.3820,  0.2885],\n",
       "          [-0.0316,  0.3167, -0.5614,  0.1216],\n",
       "          [-0.0450,  0.1769, -0.6606,  0.0872]]], grad_fn=<TopkBackward0>),\n",
       " tensor([[[2, 2, 3, 0],\n",
       "          [3, 3, 1, 1],\n",
       "          [1, 1, 0, 3]],\n",
       " \n",
       "         [[2, 2, 0, 1],\n",
       "          [1, 0, 3, 2],\n",
       "          [0, 1, 2, 0]]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_logist, topk_indices = logist.topk(top_k,dim=1)\n",
    "topk_logist, topk_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba73fd32",
   "metadata": {
    "id": "MSLU4DB71Gmq",
    "papermill": {
     "duration": 0.004871,
     "end_time": "2025-10-20T09:13:50.320012",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.315141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## -infinity And Apply Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d05cd30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.330943Z",
     "iopub.status.busy": "2025-10-20T09:13:50.330461Z",
     "iopub.status.idle": "2025-10-20T09:13:50.341226Z",
     "shell.execute_reply": "2025-10-20T09:13:50.340530Z"
    },
    "id": "01Lx2x1G03nR",
    "outputId": "4215a5b7-3fc9-40f4-9dbf-8e04d46627e7",
    "papermill": {
     "duration": 0.017344,
     "end_time": "2025-10-20T09:13:50.342296",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.324952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0696,    -inf,  0.2970, -0.6304],\n",
       "         [   -inf, -0.0116,    -inf, -0.0121],\n",
       "         [-0.8395, -0.0317,    -inf, -0.1077],\n",
       "         [   -inf,    -inf,    -inf,    -inf]],\n",
       "\n",
       "        [[-0.3820,  0.2885,  0.3378,    -inf],\n",
       "         [ 0.3167, -0.0316,  0.1216, -0.5614],\n",
       "         [ 0.0872,  0.1769, -0.6606,    -inf],\n",
       "         [   -inf,    -inf,    -inf,    -inf]]], grad_fn=<ScatterBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import inf\n",
    "zeros = torch.full_like(logist,float('-inf'))\n",
    "sparse_logist = zeros.scatter(-1, topk_indices, topk_logist)\n",
    "sparse_logist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e2f6666",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.353501Z",
     "iopub.status.busy": "2025-10-20T09:13:50.353028Z",
     "iopub.status.idle": "2025-10-20T09:13:50.361871Z",
     "shell.execute_reply": "2025-10-20T09:13:50.361309Z"
    },
    "id": "yodZYnkw2Slk",
    "outputId": "1c86cfdb-2df3-42a1-bb78-6e8b2669a253",
    "papermill": {
     "duration": 0.015521,
     "end_time": "2025-10-20T09:13:50.362958",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.347437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7128, 0.0000, 1.0000, 0.2201],\n",
       "         [0.0000, 0.5050, 0.0000, 0.4086],\n",
       "         [0.2872, 0.4950, 0.0000, 0.3713],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.2169, 0.3816, 0.4600, 0.0000],\n",
       "         [0.4363, 0.2771, 0.3706, 1.0000],\n",
       "         [0.3468, 0.3413, 0.1695, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inplace of inf we are putting zeros\n",
    "getting_output = f.softmax(sparse_logist, dim=1)\n",
    "getting_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e2382",
   "metadata": {
    "id": "OdynQ58b20oB",
    "papermill": {
     "duration": 0.004984,
     "end_time": "2025-10-20T09:13:50.373241",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.368257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Class for Topk Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa8ad1d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.384617Z",
     "iopub.status.busy": "2025-10-20T09:13:50.384084Z",
     "iopub.status.idle": "2025-10-20T09:13:50.389000Z",
     "shell.execute_reply": "2025-10-20T09:13:50.388298Z"
    },
    "id": "popTpP0H2r8b",
    "papermill": {
     "duration": 0.011729,
     "end_time": "2025-10-20T09:13:50.390135",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.378406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First define the top k router module\n",
    "class TopkRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(TopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        self.linear = nn.Linear(n_embed, num_experts)\n",
    "\n",
    "    def forward(self, mh_output):\n",
    "        # mh_output is the output tensor from multihead self-attention block\n",
    "        logits = self.linear(mh_output)\n",
    "        top_k_logits, indices = logits.topk(self.top_k, dim=-1)\n",
    "        zeros = torch.full_like(logits, float('-inf'))\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        router_output = f.softmax(sparse_logits, dim=-1)\n",
    "        return router_output, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a227371",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.401084Z",
     "iopub.status.busy": "2025-10-20T09:13:50.400849Z",
     "iopub.status.idle": "2025-10-20T09:13:50.407792Z",
     "shell.execute_reply": "2025-10-20T09:13:50.407142Z"
    },
    "id": "5gF-p-bq3nPp",
    "outputId": "257d4de5-aced-48c0-be07-f83d1bbdf1e3",
    "papermill": {
     "duration": 0.013765,
     "end_time": "2025-10-20T09:13:50.409056",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.395291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 4]),\n",
       " tensor([[[0.0000, 0.5049, 0.4951, 0.0000],\n",
       "          [0.4959, 0.0000, 0.5041, 0.0000],\n",
       "          [0.6062, 0.0000, 0.3938, 0.0000],\n",
       "          [0.5327, 0.0000, 0.4673, 0.0000]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[1, 2],\n",
       "          [2, 0],\n",
       "          [0, 2],\n",
       "          [0, 2]]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_experts=3\n",
    "top_k=2\n",
    "n_embd=8\n",
    "\n",
    "# Example\n",
    "mh_output = torch.rand(1,4,n_embed)\n",
    "top_k_gate =TopkRouter(n_embed, num_expert, top_k)\n",
    "getting_output, indices = top_k_gate(mh_output)\n",
    "\n",
    "getting_output.shape, getting_output, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc370c2b",
   "metadata": {
    "id": "Dtcc-Y1O40CH",
    "papermill": {
     "duration": 0.005755,
     "end_time": "2025-10-20T09:13:50.421541",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.415786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Noisy Top K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e190200",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.433107Z",
     "iopub.status.busy": "2025-10-20T09:13:50.432694Z",
     "iopub.status.idle": "2025-10-20T09:13:50.437986Z",
     "shell.execute_reply": "2025-10-20T09:13:50.437284Z"
    },
    "id": "5wh7faOJ4eZn",
    "papermill": {
     "duration": 0.012149,
     "end_time": "2025-10-20T09:13:50.439148",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.426999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NoisyTopkRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(NoisyTopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "\n",
    "        # layer for router logits\n",
    "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
    "        self.noise_linear = nn.Linear(n_embed, num_experts)\n",
    "\n",
    "    def forward(self, mh_output):\n",
    "        # mh_output is the output tensor from multihead self attention block\n",
    "        logits = self.topkroute_linear(mh_output)\n",
    "\n",
    "        # Noise logits\n",
    "        noise_logits = self.noise_linear(mh_output)\n",
    "\n",
    "        # Adding scaled unit Gaussian noise to the logits\n",
    "        noise = torch.randn_like(logits) * f.softplus(noise_logits)\n",
    "        noisy_logits = logits + noise\n",
    "\n",
    "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
    "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        router_output = f.softmax(sparse_logits, dim=-1)\n",
    "        return router_output, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "688c887d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.451141Z",
     "iopub.status.busy": "2025-10-20T09:13:50.450510Z",
     "iopub.status.idle": "2025-10-20T09:13:50.465303Z",
     "shell.execute_reply": "2025-10-20T09:13:50.464633Z"
    },
    "id": "AmxY2NMK5HXS",
    "outputId": "2d301367-de5c-4f6f-8056-3ff20b07a6b7",
    "papermill": {
     "duration": 0.021809,
     "end_time": "2025-10-20T09:13:50.466367",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.444558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 3]),\n",
       " tensor([[[0.0000, 0.6529, 0.3471],\n",
       "          [0.1302, 0.0000, 0.8698],\n",
       "          [0.4382, 0.0000, 0.5618],\n",
       "          [0.5730, 0.4270, 0.0000]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[1, 2],\n",
       "          [2, 0],\n",
       "          [2, 0],\n",
       "          [0, 1]]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing this out, again:\n",
    "num_experts = 3\n",
    "top_k = 2\n",
    "n_embd = 8\n",
    "\n",
    "mh_output = torch.randn(1, 4, n_embd)  # Example input\n",
    "noisy_top_k_gate = NoisyTopkRouter(n_embd, num_experts, top_k)\n",
    "gating_output, indices = noisy_top_k_gate(mh_output)\n",
    "\n",
    "gating_output.shape, gating_output, indices\n",
    "# ✅ It works!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69b26444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.478777Z",
     "iopub.status.busy": "2025-10-20T09:13:50.478259Z",
     "iopub.status.idle": "2025-10-20T09:13:50.484371Z",
     "shell.execute_reply": "2025-10-20T09:13:50.483826Z"
    },
    "id": "nUckXfyW5gIT",
    "papermill": {
     "duration": 0.013254,
     "end_time": "2025-10-20T09:13:50.485388",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.472134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
    "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        gating_output, indices = self.router(x)\n",
    "        final_output = torch.zeros_like(x)\n",
    "\n",
    "        # Reshape inputs for batch processing\n",
    "        flat_x = x.view(-1, x.size(-1))\n",
    "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
    "\n",
    "        # Process each expert in parallel\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Create a mask for the inputs where the current expert is in top-k\n",
    "            expert_mask = (indices == i).any(dim=-1)\n",
    "            flat_mask = expert_mask.view(-1)\n",
    "\n",
    "            if flat_mask.any():\n",
    "                expert_input = flat_x[flat_mask]\n",
    "                expert_output = expert(expert_input)\n",
    "\n",
    "                # Extract and apply gating scores\n",
    "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
    "                weighted_output = expert_output * gating_scores\n",
    "\n",
    "                # Update final output additively by indexing and adding\n",
    "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
    "\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d82f17f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.497200Z",
     "iopub.status.busy": "2025-10-20T09:13:50.496815Z",
     "iopub.status.idle": "2025-10-20T09:13:50.552435Z",
     "shell.execute_reply": "2025-10-20T09:13:50.551563Z"
    },
    "id": "Wy7jtqfC57UD",
    "outputId": "e73d8f8a-8a63-472f-b461-dd718e99a3dd",
    "papermill": {
     "duration": 0.062734,
     "end_time": "2025-10-20T09:13:50.553655",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.490921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the final output: torch.Size([1, 4, 8])\n",
      "tensor([[[ 0.0376, -0.0006,  0.1586,  0.0059,  0.3026, -0.0855, -0.1571,\n",
      "          -0.2406],\n",
      "         [ 0.1958,  0.0896, -0.1411, -0.0173,  0.2917, -0.0550, -0.0507,\n",
      "          -0.0014],\n",
      "         [-0.0573, -0.0445, -0.0484, -0.0455,  0.0413,  0.1064, -0.1094,\n",
      "          -0.0904],\n",
      "         [-0.0945, -0.1488, -0.0067, -0.1033, -0.1375,  0.2705,  0.0921,\n",
      "          -0.2506]]], grad_fn=<IndexPutBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's test this out\n",
    "num_experts = 3\n",
    "top_k = 2\n",
    "n_embd = 8\n",
    "dropout = 0.1\n",
    "\n",
    "mh_output = torch.randn(1, 4, n_embd)  # Example multi-head attention output\n",
    "sparse_moe = SparseMoE(n_embd, num_experts, top_k)\n",
    "final_output = sparse_moe(mh_output)\n",
    "\n",
    "print(\"Shape of the final output:\", final_output.shape)\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87877cc2",
   "metadata": {
    "id": "3QYbkHAY56Ka",
    "papermill": {
     "duration": 0.005458,
     "end_time": "2025-10-20T09:13:50.565460",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.560002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f563cc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.577544Z",
     "iopub.status.busy": "2025-10-20T09:13:50.577270Z",
     "iopub.status.idle": "2025-10-20T09:13:50.586796Z",
     "shell.execute_reply": "2025-10-20T09:13:50.586096Z"
    },
    "id": "DE3dp-rs5oYA",
    "outputId": "aeff3ff6-ddbf-465d-88a3-b04b25319b11",
    "papermill": {
     "duration": 0.016806,
     "end_time": "2025-10-20T09:13:50.587838",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.571032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NoisyTopkRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(NoisyTopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        # layer for router logits\n",
    "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
    "        self.noise_linear = nn.Linear(n_embed, num_experts)\n",
    "\n",
    "    def forward(self, mh_output):\n",
    "        # mh_output is the output tensor from multihead self attention block\n",
    "        logits = self.topkroute_linear(mh_output)\n",
    "\n",
    "        # Noise logits\n",
    "        noise_logits = self.noise_linear(mh_output)\n",
    "\n",
    "        # Adding scaled unit Gaussian noise to the logits\n",
    "        noise = torch.randn_like(logits) * F.softplus(noise_logits)\n",
    "        noisy_logits = logits + noise\n",
    "\n",
    "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
    "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        router_output = F.softmax(sparse_logits, dim=-1)\n",
    "        return router_output, indices\n",
    "\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super(Expert, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed, n_embed)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
    "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        gating_output, indices = self.router(x)\n",
    "        final_output = torch.zeros_like(x)\n",
    "\n",
    "        # Reshape inputs for batch processing\n",
    "        flat_x = x.view(-1, x.size(-1))\n",
    "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
    "\n",
    "        # Process each expert in parallel\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Create a mask for the inputs where the current expert is in top-k\n",
    "            expert_mask = (indices == i).any(dim=-1)\n",
    "            flat_mask = expert_mask.view(-1)\n",
    "\n",
    "            if flat_mask.any():\n",
    "                expert_input = flat_x[flat_mask]\n",
    "                expert_output = expert(expert_input)\n",
    "\n",
    "                # Extract and apply gating scores\n",
    "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
    "                weighted_output = expert_output * gating_scores\n",
    "\n",
    "                # Update final output additively by indexing and adding\n",
    "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6553b27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.600026Z",
     "iopub.status.busy": "2025-10-20T09:13:50.599368Z",
     "iopub.status.idle": "2025-10-20T09:13:50.611631Z",
     "shell.execute_reply": "2025-10-20T09:13:50.610921Z"
    },
    "id": "38qwZyB561Jc",
    "outputId": "6026af29-a838-43fb-a93e-6eaa05644427",
    "papermill": {
     "duration": 0.019239,
     "end_time": "2025-10-20T09:13:50.612668",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.593429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the final output: torch.Size([1, 4, 8])\n",
      "tensor([[[ 0.3192, -0.0379,  0.0903,  0.1086,  0.1156,  0.2662, -0.0802,\n",
      "          -0.1442],\n",
      "         [ 0.0494, -0.0442, -0.2182,  0.0866, -0.0322, -0.0541,  0.0818,\n",
      "          -0.3449],\n",
      "         [-0.0343,  0.4192,  0.0721,  0.2563,  0.2353,  0.3127, -0.2235,\n",
      "           0.0469],\n",
      "         [-0.2914,  0.3868, -0.0984,  0.1426,  0.4994,  0.4653, -0.3880,\n",
      "          -0.3570]]], grad_fn=<IndexPutBackward0>)\n"
     ]
    }
   ],
   "source": [
    "num_experts = 3\n",
    "top_k = 2\n",
    "n_embd = 8\n",
    "dropout = 0.1\n",
    "\n",
    "mh_output = torch.randn(1, 4, n_embd)  # Example multi-head attention output\n",
    "sparse_moe = SparseMoE(n_embd, num_experts, top_k)\n",
    "final_output = sparse_moe(mh_output)\n",
    "\n",
    "print(\"Shape of the final output:\", final_output.shape)\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef896ebe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.624925Z",
     "iopub.status.busy": "2025-10-20T09:13:50.624699Z",
     "iopub.status.idle": "2025-10-20T09:13:50.630292Z",
     "shell.execute_reply": "2025-10-20T09:13:50.629752Z"
    },
    "id": "aP9CR6eO7iz6",
    "papermill": {
     "duration": 0.012826,
     "end_time": "2025-10-20T09:13:50.631291",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.618465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)      # (B, T, C)\n",
    "        q = self.query(x)    # (B, T, C)\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5      # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)                 # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)                            # (B, T, C)\n",
    "        out = wei @ v                                # (B, T, C)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90aaef0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.643360Z",
     "iopub.status.busy": "2025-10-20T09:13:50.643159Z",
     "iopub.status.idle": "2025-10-20T09:13:50.647448Z",
     "shell.execute_reply": "2025-10-20T09:13:50.646950Z"
    },
    "id": "FgDSm2zv7ktb",
    "papermill": {
     "duration": 0.011431,
     "end_time": "2025-10-20T09:13:50.648417",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.636986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Multi-Headed Self Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embed, head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe93a7b",
   "metadata": {
    "id": "lqfuUtw_7bs7",
    "papermill": {
     "duration": 0.005357,
     "end_time": "2025-10-20T09:13:50.659457",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.654100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4daa7d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.671762Z",
     "iopub.status.busy": "2025-10-20T09:13:50.671543Z",
     "iopub.status.idle": "2025-10-20T09:13:50.675827Z",
     "shell.execute_reply": "2025-10-20T09:13:50.675328Z"
    },
    "id": "rLt2_gKk65dD",
    "papermill": {
     "duration": 0.011455,
     "end_time": "2025-10-20T09:13:50.676876",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.665421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Mixture of Experts Transformer block: communication followed by computation (multi-head self attention) \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head, num_experts, top_k):\n",
    "        # n_embed: embedding dimension, n_head: number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_embed, n_head, head_size)\n",
    "        self.smoe = SparseMoE(n_embed, num_experts, top_k)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.smoe(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41036dcd",
   "metadata": {
    "id": "dMSHPB_e70BR",
    "papermill": {
     "duration": 0.005423,
     "end_time": "2025-10-20T09:13:50.688027",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.682604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4916d08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.700451Z",
     "iopub.status.busy": "2025-10-20T09:13:50.700001Z",
     "iopub.status.idle": "2025-10-20T09:13:50.707469Z",
     "shell.execute_reply": "2025-10-20T09:13:50.706923Z"
    },
    "id": "N--f3vba7ziR",
    "papermill": {
     "duration": 0.014835,
     "end_time": "2025-10-20T09:13:50.708479",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.693644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# assume these globals are defined somewhere in your script\n",
    "# vocab_size, n_embed, block_size, n_head, n_layer, num_experts, top_k, device = ...\n",
    "# and classes: Block (uses MultiHeadAttention + SparseMoE)\n",
    "\n",
    "class SparseMoELanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table   = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embed, n_head, num_experts=num_experts, top_k=top_k)\n",
    "              for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embed)            # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)                         # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb                                             # (B,T,C)\n",
    "        x = self.blocks(x)                                                # (B,T,C)\n",
    "        x = self.ln_f(x)                                                  # (B,T,C)\n",
    "        logits = self.lm_head(x)                                          # (B,T,V)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Autoregressively sample next tokens.\n",
    "        idx: LongTensor of shape (B, T)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]                 # (B, T_ctx)\n",
    "            logits, _ = self(idx_cond)                      # (B, T_ctx, V)\n",
    "            logits = logits[:, -1, :]                       # (B, V)\n",
    "            probs = F.softmax(logits, dim=-1)               # (B, V)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)         # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee62e59f",
   "metadata": {
    "id": "5GHYpiJC8uAd",
    "papermill": {
     "duration": 0.005418,
     "end_time": "2025-10-20T09:13:50.719754",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.714336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "348a4c4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.732100Z",
     "iopub.status.busy": "2025-10-20T09:13:50.731614Z",
     "iopub.status.idle": "2025-10-20T09:13:50.881874Z",
     "shell.execute_reply": "2025-10-20T09:13:50.881289Z"
    },
    "id": "VrSGWaa98sa6",
    "outputId": "474db61a-9dc7-40fb-cdce-8c9694cb1524",
    "papermill": {
     "duration": 0.157757,
     "end_time": "2025-10-20T09:13:50.883151",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.725394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Read the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# --- Create character-level vocabulary ---\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "# Mappings (char ↔ index)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]                # string → list of ints\n",
    "decode = lambda l: ''.join([itos[i] for i in l])       # list of ints → string\n",
    "\n",
    "# --- Train / Validation Split ---\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))  # first 90% train, rest validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# --- Dataloader Function ---\n",
    "def get_batch(split):\n",
    "    \"\"\"Generate a small batch of data for inputs (x) and targets (y).\"\"\"\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dc3925",
   "metadata": {
    "id": "1qI-cQ0B9Hmr",
    "papermill": {
     "duration": 0.005709,
     "end_time": "2025-10-20T09:13:50.895157",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.889448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define LLM loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "493e832d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.907600Z",
     "iopub.status.busy": "2025-10-20T09:13:50.907195Z",
     "iopub.status.idle": "2025-10-20T09:13:50.911566Z",
     "shell.execute_reply": "2025-10-20T09:13:50.911015Z"
    },
    "id": "Cd0oEvrL9HBL",
    "papermill": {
     "duration": 0.011636,
     "end_time": "2025-10-20T09:13:50.912505",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.900869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22f053",
   "metadata": {
    "id": "UVw10Dkw9Y4S",
    "papermill": {
     "duration": 0.005843,
     "end_time": "2025-10-20T09:13:50.924314",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.918471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training loop pararms and hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1aba582d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:50.937421Z",
     "iopub.status.busy": "2025-10-20T09:13:50.936833Z",
     "iopub.status.idle": "2025-10-20T09:13:51.009231Z",
     "shell.execute_reply": "2025-10-20T09:13:51.008571Z"
    },
    "id": "pfgzAoct9Wfv",
    "papermill": {
     "duration": 0.080025,
     "end_time": "2025-10-20T09:13:51.010308",
     "exception": false,
     "start_time": "2025-10-20T09:13:50.930283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 14: Define training loop parameters and other hyperparameters\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "# ----------------\n",
    "# Hyperparameters\n",
    "# ----------------\n",
    "batch_size = 32          # how many independent sequences will we process in parallel?\n",
    "block_size = 64         # what is the maximum context length for predictions?\n",
    "max_iters = 60000         # total training iterations  increase this if you want accurate result 60k 100k\n",
    "eval_interval = 100      # evaluate the model every N steps\n",
    "learning_rate = 1e-3     # optimizer learning rate\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 400         # number of iterations for evaluation\n",
    "head_size = 16\n",
    "n_embed = 128\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.1\n",
    "num_experts = 8\n",
    "top_k = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a6596",
   "metadata": {
    "id": "qT8-4tQV9lWJ",
    "papermill": {
     "duration": 0.005702,
     "end_time": "2025-10-20T09:13:51.022510",
     "exception": false,
     "start_time": "2025-10-20T09:13:51.016808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "761a64d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:51.035046Z",
     "iopub.status.busy": "2025-10-20T09:13:51.034697Z",
     "iopub.status.idle": "2025-10-20T09:13:51.118563Z",
     "shell.execute_reply": "2025-10-20T09:13:51.117721Z"
    },
    "id": "JbQt1sEb9nIT",
    "outputId": "01dcb50c-16df-49e1-9e5d-6be29f358bac",
    "papermill": {
     "duration": 0.091673,
     "end_time": "2025-10-20T09:13:51.120006",
     "exception": false,
     "start_time": "2025-10-20T09:13:51.028333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseMoELanguageModel(\n",
       "  (token_embedding_table): Embedding(65, 128)\n",
       "  (position_embedding_table): Embedding(64, 128)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=128, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kaiming_init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "model = SparseMoELanguageModel()\n",
    "model.apply(kaiming_init_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb29d91",
   "metadata": {
    "id": "BYcsJj_3-BsW",
    "papermill": {
     "duration": 0.006087,
     "end_time": "2025-10-20T09:13:51.132730",
     "exception": false,
     "start_time": "2025-10-20T09:13:51.126643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77c8ba65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T09:13:51.146252Z",
     "iopub.status.busy": "2025-10-20T09:13:51.145649Z",
     "iopub.status.idle": "2025-10-20T19:22:36.282858Z",
     "shell.execute_reply": "2025-10-20T19:22:36.281964Z"
    },
    "id": "NXjXeD1f97lX",
    "outputId": "7a3cf4dd-ea7c-4d4c-e57c-311b8de667ff",
    "papermill": {
     "duration": 36525.178098,
     "end_time": "2025-10-20T19:22:36.317048",
     "exception": false,
     "start_time": "2025-10-20T09:13:51.138950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.684609 M parameters\n",
      "step 0: train loss 5.2896, val loss 5.2803\n",
      "step 100: train loss 2.5812, val loss 2.5753\n",
      "step 200: train loss 2.4298, val loss 2.4514\n",
      "step 300: train loss 2.3251, val loss 2.3461\n",
      "step 400: train loss 2.2236, val loss 2.2570\n",
      "step 500: train loss 2.1160, val loss 2.1619\n",
      "step 600: train loss 2.0220, val loss 2.1018\n",
      "step 700: train loss 1.9495, val loss 2.0407\n",
      "step 800: train loss 1.8903, val loss 1.9984\n",
      "step 900: train loss 1.8227, val loss 1.9475\n",
      "step 1000: train loss 1.7839, val loss 1.9317\n",
      "step 1100: train loss 1.7430, val loss 1.8949\n",
      "step 1200: train loss 1.7066, val loss 1.8586\n",
      "step 1300: train loss 1.6747, val loss 1.8386\n",
      "step 1400: train loss 1.6580, val loss 1.8276\n",
      "step 1500: train loss 1.6298, val loss 1.7831\n",
      "step 1600: train loss 1.6076, val loss 1.7638\n",
      "step 1700: train loss 1.5885, val loss 1.7630\n",
      "step 1800: train loss 1.5778, val loss 1.7549\n",
      "step 1900: train loss 1.5546, val loss 1.7280\n",
      "step 2000: train loss 1.5441, val loss 1.7262\n",
      "step 2100: train loss 1.5313, val loss 1.7020\n",
      "step 2200: train loss 1.5225, val loss 1.6895\n",
      "step 2300: train loss 1.5044, val loss 1.6913\n",
      "step 2400: train loss 1.4998, val loss 1.6758\n",
      "step 2500: train loss 1.4869, val loss 1.6778\n",
      "step 2600: train loss 1.4750, val loss 1.6612\n",
      "step 2700: train loss 1.4648, val loss 1.6544\n",
      "step 2800: train loss 1.4614, val loss 1.6531\n",
      "step 2900: train loss 1.4588, val loss 1.6639\n",
      "step 3000: train loss 1.4499, val loss 1.6512\n",
      "step 3100: train loss 1.4416, val loss 1.6463\n",
      "step 3200: train loss 1.4348, val loss 1.6413\n",
      "step 3300: train loss 1.4330, val loss 1.6277\n",
      "step 3400: train loss 1.4247, val loss 1.6223\n",
      "step 3500: train loss 1.4175, val loss 1.6194\n",
      "step 3600: train loss 1.4153, val loss 1.6151\n",
      "step 3700: train loss 1.4101, val loss 1.6098\n",
      "step 3800: train loss 1.4021, val loss 1.6174\n",
      "step 3900: train loss 1.3977, val loss 1.6098\n",
      "step 4000: train loss 1.3903, val loss 1.6071\n",
      "step 4100: train loss 1.3898, val loss 1.6043\n",
      "step 4200: train loss 1.3838, val loss 1.5952\n",
      "step 4300: train loss 1.3841, val loss 1.5967\n",
      "step 4400: train loss 1.3809, val loss 1.5969\n",
      "step 4500: train loss 1.3706, val loss 1.5896\n",
      "step 4600: train loss 1.3754, val loss 1.5894\n",
      "step 4700: train loss 1.3604, val loss 1.5801\n",
      "step 4800: train loss 1.3669, val loss 1.5776\n",
      "step 4900: train loss 1.3593, val loss 1.5800\n",
      "step 5000: train loss 1.3580, val loss 1.5673\n",
      "step 5100: train loss 1.3543, val loss 1.5682\n",
      "step 5200: train loss 1.3541, val loss 1.5745\n",
      "step 5300: train loss 1.3482, val loss 1.5767\n",
      "step 5400: train loss 1.3473, val loss 1.5712\n",
      "step 5500: train loss 1.3391, val loss 1.5635\n",
      "step 5600: train loss 1.3384, val loss 1.5645\n",
      "step 5700: train loss 1.3368, val loss 1.5599\n",
      "step 5800: train loss 1.3326, val loss 1.5651\n",
      "step 5900: train loss 1.3288, val loss 1.5693\n",
      "step 6000: train loss 1.3286, val loss 1.5601\n",
      "step 6100: train loss 1.3210, val loss 1.5623\n",
      "step 6200: train loss 1.3243, val loss 1.5656\n",
      "step 6300: train loss 1.3153, val loss 1.5614\n",
      "step 6400: train loss 1.3190, val loss 1.5491\n",
      "step 6500: train loss 1.3132, val loss 1.5562\n",
      "step 6600: train loss 1.3183, val loss 1.5597\n",
      "step 6700: train loss 1.3095, val loss 1.5470\n",
      "step 6800: train loss 1.3070, val loss 1.5524\n",
      "step 6900: train loss 1.3046, val loss 1.5435\n",
      "step 7000: train loss 1.3040, val loss 1.5534\n",
      "step 7100: train loss 1.3002, val loss 1.5483\n",
      "step 7200: train loss 1.2983, val loss 1.5535\n",
      "step 7300: train loss 1.2932, val loss 1.5440\n",
      "step 7400: train loss 1.2971, val loss 1.5510\n",
      "step 7500: train loss 1.2961, val loss 1.5463\n",
      "step 7600: train loss 1.2888, val loss 1.5440\n",
      "step 7700: train loss 1.2936, val loss 1.5422\n",
      "step 7800: train loss 1.2881, val loss 1.5481\n",
      "step 7900: train loss 1.2854, val loss 1.5465\n",
      "step 8000: train loss 1.2801, val loss 1.5430\n",
      "step 8100: train loss 1.2798, val loss 1.5475\n",
      "step 8200: train loss 1.2845, val loss 1.5498\n",
      "step 8300: train loss 1.2748, val loss 1.5435\n",
      "step 8400: train loss 1.2736, val loss 1.5393\n",
      "step 8500: train loss 1.2810, val loss 1.5336\n",
      "step 8600: train loss 1.2704, val loss 1.5377\n",
      "step 8700: train loss 1.2693, val loss 1.5385\n",
      "step 8800: train loss 1.2725, val loss 1.5410\n",
      "step 8900: train loss 1.2681, val loss 1.5369\n",
      "step 9000: train loss 1.2667, val loss 1.5391\n",
      "step 9100: train loss 1.2639, val loss 1.5432\n",
      "step 9200: train loss 1.2634, val loss 1.5329\n",
      "step 9300: train loss 1.2618, val loss 1.5457\n",
      "step 9400: train loss 1.2635, val loss 1.5382\n",
      "step 9500: train loss 1.2597, val loss 1.5313\n",
      "step 9600: train loss 1.2566, val loss 1.5366\n",
      "step 9700: train loss 1.2572, val loss 1.5332\n",
      "step 9800: train loss 1.2503, val loss 1.5359\n",
      "step 9900: train loss 1.2495, val loss 1.5381\n",
      "step 10000: train loss 1.2494, val loss 1.5346\n",
      "step 10100: train loss 1.2476, val loss 1.5395\n",
      "step 10200: train loss 1.2459, val loss 1.5283\n",
      "step 10300: train loss 1.2409, val loss 1.5380\n",
      "step 10400: train loss 1.2452, val loss 1.5365\n",
      "step 10500: train loss 1.2396, val loss 1.5322\n",
      "step 10600: train loss 1.2441, val loss 1.5420\n",
      "step 10700: train loss 1.2388, val loss 1.5256\n",
      "step 10800: train loss 1.2408, val loss 1.5251\n",
      "step 10900: train loss 1.2394, val loss 1.5303\n",
      "step 11000: train loss 1.2347, val loss 1.5300\n",
      "step 11100: train loss 1.2330, val loss 1.5339\n",
      "step 11200: train loss 1.2301, val loss 1.5300\n",
      "step 11300: train loss 1.2321, val loss 1.5367\n",
      "step 11400: train loss 1.2278, val loss 1.5287\n",
      "step 11500: train loss 1.2275, val loss 1.5220\n",
      "step 11600: train loss 1.2289, val loss 1.5237\n",
      "step 11700: train loss 1.2235, val loss 1.5410\n",
      "step 11800: train loss 1.2190, val loss 1.5297\n",
      "step 11900: train loss 1.2198, val loss 1.5323\n",
      "step 12000: train loss 1.2236, val loss 1.5322\n",
      "step 12100: train loss 1.2198, val loss 1.5258\n",
      "step 12200: train loss 1.2192, val loss 1.5262\n",
      "step 12300: train loss 1.2167, val loss 1.5291\n",
      "step 12400: train loss 1.2200, val loss 1.5175\n",
      "step 12500: train loss 1.2124, val loss 1.5250\n",
      "step 12600: train loss 1.2141, val loss 1.5294\n",
      "step 12700: train loss 1.2120, val loss 1.5283\n",
      "step 12800: train loss 1.2104, val loss 1.5307\n",
      "step 12900: train loss 1.2066, val loss 1.5301\n",
      "step 13000: train loss 1.2064, val loss 1.5301\n",
      "step 13100: train loss 1.2089, val loss 1.5358\n",
      "step 13200: train loss 1.2063, val loss 1.5365\n",
      "step 13300: train loss 1.2027, val loss 1.5415\n",
      "step 13400: train loss 1.2050, val loss 1.5381\n",
      "step 13500: train loss 1.2028, val loss 1.5393\n",
      "step 13600: train loss 1.2038, val loss 1.5400\n",
      "step 13700: train loss 1.1984, val loss 1.5278\n",
      "step 13800: train loss 1.2063, val loss 1.5356\n",
      "step 13900: train loss 1.2007, val loss 1.5293\n",
      "step 14000: train loss 1.1961, val loss 1.5213\n",
      "step 14100: train loss 1.1957, val loss 1.5292\n",
      "step 14200: train loss 1.1946, val loss 1.5359\n",
      "step 14300: train loss 1.1920, val loss 1.5380\n",
      "step 14400: train loss 1.1931, val loss 1.5279\n",
      "step 14500: train loss 1.1891, val loss 1.5251\n",
      "step 14600: train loss 1.1925, val loss 1.5410\n",
      "step 14700: train loss 1.1904, val loss 1.5354\n",
      "step 14800: train loss 1.1836, val loss 1.5327\n",
      "step 14900: train loss 1.1836, val loss 1.5326\n",
      "step 15000: train loss 1.1873, val loss 1.5336\n",
      "step 15100: train loss 1.1885, val loss 1.5397\n",
      "step 15200: train loss 1.1842, val loss 1.5214\n",
      "step 15300: train loss 1.1805, val loss 1.5226\n",
      "step 15400: train loss 1.1799, val loss 1.5318\n",
      "step 15500: train loss 1.1788, val loss 1.5287\n",
      "step 15600: train loss 1.1793, val loss 1.5297\n",
      "step 15700: train loss 1.1769, val loss 1.5239\n",
      "step 15800: train loss 1.1765, val loss 1.5301\n",
      "step 15900: train loss 1.1757, val loss 1.5381\n",
      "step 16000: train loss 1.1734, val loss 1.5371\n",
      "step 16100: train loss 1.1734, val loss 1.5310\n",
      "step 16200: train loss 1.1683, val loss 1.5283\n",
      "step 16300: train loss 1.1681, val loss 1.5300\n",
      "step 16400: train loss 1.1676, val loss 1.5325\n",
      "step 16500: train loss 1.1689, val loss 1.5351\n",
      "step 16600: train loss 1.1667, val loss 1.5393\n",
      "step 16700: train loss 1.1703, val loss 1.5372\n",
      "step 16800: train loss 1.1674, val loss 1.5359\n",
      "step 16900: train loss 1.1641, val loss 1.5388\n",
      "step 17000: train loss 1.1645, val loss 1.5375\n",
      "step 17100: train loss 1.1589, val loss 1.5461\n",
      "step 17200: train loss 1.1593, val loss 1.5310\n",
      "step 17300: train loss 1.1606, val loss 1.5342\n",
      "step 17400: train loss 1.1592, val loss 1.5362\n",
      "step 17500: train loss 1.1575, val loss 1.5438\n",
      "step 17600: train loss 1.1563, val loss 1.5369\n",
      "step 17700: train loss 1.1579, val loss 1.5381\n",
      "step 17800: train loss 1.1557, val loss 1.5394\n",
      "step 17900: train loss 1.1551, val loss 1.5241\n",
      "step 18000: train loss 1.1510, val loss 1.5382\n",
      "step 18100: train loss 1.1520, val loss 1.5360\n",
      "step 18200: train loss 1.1531, val loss 1.5395\n",
      "step 18300: train loss 1.1507, val loss 1.5318\n",
      "step 18400: train loss 1.1461, val loss 1.5438\n",
      "step 18500: train loss 1.1507, val loss 1.5450\n",
      "step 18600: train loss 1.1467, val loss 1.5406\n",
      "step 18700: train loss 1.1473, val loss 1.5386\n",
      "step 18800: train loss 1.1437, val loss 1.5412\n",
      "step 18900: train loss 1.1417, val loss 1.5511\n",
      "step 19000: train loss 1.1423, val loss 1.5431\n",
      "step 19100: train loss 1.1416, val loss 1.5465\n",
      "step 19200: train loss 1.1417, val loss 1.5419\n",
      "step 19300: train loss 1.1410, val loss 1.5489\n",
      "step 19400: train loss 1.1365, val loss 1.5400\n",
      "step 19500: train loss 1.1386, val loss 1.5465\n",
      "step 19600: train loss 1.1330, val loss 1.5349\n",
      "step 19700: train loss 1.1329, val loss 1.5468\n",
      "step 19800: train loss 1.1317, val loss 1.5434\n",
      "step 19900: train loss 1.1319, val loss 1.5440\n",
      "step 20000: train loss 1.1328, val loss 1.5382\n",
      "step 20100: train loss 1.1300, val loss 1.5470\n",
      "step 20200: train loss 1.1279, val loss 1.5441\n",
      "step 20300: train loss 1.1292, val loss 1.5444\n",
      "step 20400: train loss 1.1273, val loss 1.5476\n",
      "step 20500: train loss 1.1261, val loss 1.5383\n",
      "step 20600: train loss 1.1286, val loss 1.5509\n",
      "step 20700: train loss 1.1262, val loss 1.5426\n",
      "step 20800: train loss 1.1235, val loss 1.5445\n",
      "step 20900: train loss 1.1257, val loss 1.5503\n",
      "step 21000: train loss 1.1231, val loss 1.5625\n",
      "step 21100: train loss 1.1215, val loss 1.5506\n",
      "step 21200: train loss 1.1203, val loss 1.5543\n",
      "step 21300: train loss 1.1203, val loss 1.5571\n",
      "step 21400: train loss 1.1200, val loss 1.5501\n",
      "step 21500: train loss 1.1149, val loss 1.5482\n",
      "step 21600: train loss 1.1145, val loss 1.5519\n",
      "step 21700: train loss 1.1178, val loss 1.5583\n",
      "step 21800: train loss 1.1147, val loss 1.5558\n",
      "step 21900: train loss 1.1099, val loss 1.5538\n",
      "step 22000: train loss 1.1121, val loss 1.5595\n",
      "step 22100: train loss 1.1127, val loss 1.5494\n",
      "step 22200: train loss 1.1104, val loss 1.5524\n",
      "step 22300: train loss 1.1103, val loss 1.5588\n",
      "step 22400: train loss 1.1080, val loss 1.5559\n",
      "step 22500: train loss 1.1100, val loss 1.5563\n",
      "step 22600: train loss 1.1095, val loss 1.5495\n",
      "step 22700: train loss 1.1087, val loss 1.5568\n",
      "step 22800: train loss 1.1070, val loss 1.5592\n",
      "step 22900: train loss 1.1041, val loss 1.5572\n",
      "step 23000: train loss 1.1036, val loss 1.5691\n",
      "step 23100: train loss 1.1046, val loss 1.5529\n",
      "step 23200: train loss 1.1023, val loss 1.5616\n",
      "step 23300: train loss 1.1011, val loss 1.5723\n",
      "step 23400: train loss 1.1037, val loss 1.5678\n",
      "step 23500: train loss 1.1006, val loss 1.5556\n",
      "step 23600: train loss 1.0980, val loss 1.5536\n",
      "step 23700: train loss 1.0979, val loss 1.5644\n",
      "step 23800: train loss 1.0971, val loss 1.5618\n",
      "step 23900: train loss 1.0961, val loss 1.5647\n",
      "step 24000: train loss 1.0933, val loss 1.5657\n",
      "step 24100: train loss 1.0943, val loss 1.5617\n",
      "step 24200: train loss 1.0946, val loss 1.5552\n",
      "step 24300: train loss 1.0930, val loss 1.5599\n",
      "step 24400: train loss 1.0900, val loss 1.5692\n",
      "step 24500: train loss 1.0886, val loss 1.5600\n",
      "step 24600: train loss 1.0887, val loss 1.5515\n",
      "step 24700: train loss 1.0902, val loss 1.5595\n",
      "step 24800: train loss 1.0902, val loss 1.5701\n",
      "step 24900: train loss 1.0893, val loss 1.5731\n",
      "step 25000: train loss 1.0884, val loss 1.5635\n",
      "step 25100: train loss 1.0843, val loss 1.5676\n",
      "step 25200: train loss 1.0838, val loss 1.5684\n",
      "step 25300: train loss 1.0807, val loss 1.5557\n",
      "step 25400: train loss 1.0828, val loss 1.5796\n",
      "step 25500: train loss 1.0829, val loss 1.5741\n",
      "step 25600: train loss 1.0812, val loss 1.5726\n",
      "step 25700: train loss 1.0789, val loss 1.5719\n",
      "step 25800: train loss 1.0807, val loss 1.5666\n",
      "step 25900: train loss 1.0801, val loss 1.5701\n",
      "step 26000: train loss 1.0774, val loss 1.5632\n",
      "step 26100: train loss 1.0741, val loss 1.5631\n",
      "step 26200: train loss 1.0782, val loss 1.5715\n",
      "step 26300: train loss 1.0757, val loss 1.5733\n",
      "step 26400: train loss 1.0758, val loss 1.5805\n",
      "step 26500: train loss 1.0694, val loss 1.5678\n",
      "step 26600: train loss 1.0722, val loss 1.5699\n",
      "step 26700: train loss 1.0705, val loss 1.5718\n",
      "step 26800: train loss 1.0714, val loss 1.5816\n",
      "step 26900: train loss 1.0684, val loss 1.5738\n",
      "step 27000: train loss 1.0672, val loss 1.5784\n",
      "step 27100: train loss 1.0734, val loss 1.5720\n",
      "step 27200: train loss 1.0661, val loss 1.5736\n",
      "step 27300: train loss 1.0679, val loss 1.5785\n",
      "step 27400: train loss 1.0689, val loss 1.5745\n",
      "step 27500: train loss 1.0662, val loss 1.5822\n",
      "step 27600: train loss 1.0669, val loss 1.5851\n",
      "step 27700: train loss 1.0624, val loss 1.5891\n",
      "step 27800: train loss 1.0633, val loss 1.5797\n",
      "step 27900: train loss 1.0617, val loss 1.5855\n",
      "step 28000: train loss 1.0590, val loss 1.5814\n",
      "step 28100: train loss 1.0633, val loss 1.5803\n",
      "step 28200: train loss 1.0610, val loss 1.5778\n",
      "step 28300: train loss 1.0595, val loss 1.5935\n",
      "step 28400: train loss 1.0626, val loss 1.5860\n",
      "step 28500: train loss 1.0594, val loss 1.5758\n",
      "step 28600: train loss 1.0554, val loss 1.5818\n",
      "step 28700: train loss 1.0539, val loss 1.5888\n",
      "step 28800: train loss 1.0554, val loss 1.5900\n",
      "step 28900: train loss 1.0551, val loss 1.5938\n",
      "step 29000: train loss 1.0529, val loss 1.5916\n",
      "step 29100: train loss 1.0521, val loss 1.5978\n",
      "step 29200: train loss 1.0530, val loss 1.5928\n",
      "step 29300: train loss 1.0511, val loss 1.5826\n",
      "step 29400: train loss 1.0487, val loss 1.5798\n",
      "step 29500: train loss 1.0480, val loss 1.5787\n",
      "step 29600: train loss 1.0482, val loss 1.5914\n",
      "step 29700: train loss 1.0461, val loss 1.5944\n",
      "step 29800: train loss 1.0446, val loss 1.5803\n",
      "step 29900: train loss 1.0441, val loss 1.5965\n",
      "step 30000: train loss 1.0416, val loss 1.6005\n",
      "step 30100: train loss 1.0434, val loss 1.5967\n",
      "step 30200: train loss 1.0449, val loss 1.6000\n",
      "step 30300: train loss 1.0446, val loss 1.5886\n",
      "step 30400: train loss 1.0442, val loss 1.5933\n",
      "step 30500: train loss 1.0436, val loss 1.6002\n",
      "step 30600: train loss 1.0416, val loss 1.6042\n",
      "step 30700: train loss 1.0402, val loss 1.6038\n",
      "step 30800: train loss 1.0390, val loss 1.6031\n",
      "step 30900: train loss 1.0366, val loss 1.6016\n",
      "step 31000: train loss 1.0384, val loss 1.5962\n",
      "step 31100: train loss 1.0363, val loss 1.5945\n",
      "step 31200: train loss 1.0381, val loss 1.6009\n",
      "step 31300: train loss 1.0319, val loss 1.6103\n",
      "step 31400: train loss 1.0324, val loss 1.6195\n",
      "step 31500: train loss 1.0320, val loss 1.5945\n",
      "step 31600: train loss 1.0350, val loss 1.5968\n",
      "step 31700: train loss 1.0298, val loss 1.6044\n",
      "step 31800: train loss 1.0304, val loss 1.6056\n",
      "step 31900: train loss 1.0307, val loss 1.6107\n",
      "step 32000: train loss 1.0283, val loss 1.5954\n",
      "step 32100: train loss 1.0297, val loss 1.6223\n",
      "step 32200: train loss 1.0294, val loss 1.6113\n",
      "step 32300: train loss 1.0250, val loss 1.6077\n",
      "step 32400: train loss 1.0261, val loss 1.6092\n",
      "step 32500: train loss 1.0229, val loss 1.6122\n",
      "step 32600: train loss 1.0256, val loss 1.6041\n",
      "step 32700: train loss 1.0219, val loss 1.6130\n",
      "step 32800: train loss 1.0246, val loss 1.6135\n",
      "step 32900: train loss 1.0205, val loss 1.6176\n",
      "step 33000: train loss 1.0205, val loss 1.6052\n",
      "step 33100: train loss 1.0216, val loss 1.6127\n",
      "step 33200: train loss 1.0224, val loss 1.6247\n",
      "step 33300: train loss 1.0206, val loss 1.6210\n",
      "step 33400: train loss 1.0229, val loss 1.6206\n",
      "step 33500: train loss 1.0181, val loss 1.6191\n",
      "step 33600: train loss 1.0198, val loss 1.6076\n",
      "step 33700: train loss 1.0136, val loss 1.6134\n",
      "step 33800: train loss 1.0140, val loss 1.5996\n",
      "step 33900: train loss 1.0104, val loss 1.6170\n",
      "step 34000: train loss 1.0162, val loss 1.6124\n",
      "step 34100: train loss 1.0129, val loss 1.6173\n",
      "step 34200: train loss 1.0143, val loss 1.6221\n",
      "step 34300: train loss 1.0106, val loss 1.6264\n",
      "step 34400: train loss 1.0109, val loss 1.6262\n",
      "step 34500: train loss 1.0081, val loss 1.6145\n",
      "step 34600: train loss 1.0077, val loss 1.6203\n",
      "step 34700: train loss 1.0089, val loss 1.6145\n",
      "step 34800: train loss 1.0063, val loss 1.6230\n",
      "step 34900: train loss 1.0101, val loss 1.6242\n",
      "step 35000: train loss 1.0094, val loss 1.6287\n",
      "step 35100: train loss 1.0048, val loss 1.6299\n",
      "step 35200: train loss 0.9985, val loss 1.6386\n",
      "step 35300: train loss 1.0040, val loss 1.6257\n",
      "step 35400: train loss 1.0051, val loss 1.6123\n",
      "step 35500: train loss 1.0046, val loss 1.6340\n",
      "step 35600: train loss 1.0043, val loss 1.6341\n",
      "step 35700: train loss 0.9963, val loss 1.6360\n",
      "step 35800: train loss 1.0053, val loss 1.6321\n",
      "step 35900: train loss 0.9996, val loss 1.6232\n",
      "step 36000: train loss 1.0017, val loss 1.6403\n",
      "step 36100: train loss 1.0005, val loss 1.6235\n",
      "step 36200: train loss 0.9938, val loss 1.6401\n",
      "step 36300: train loss 0.9990, val loss 1.6235\n",
      "step 36400: train loss 0.9954, val loss 1.6324\n",
      "step 36500: train loss 0.9958, val loss 1.6191\n",
      "step 36600: train loss 0.9958, val loss 1.6197\n",
      "step 36700: train loss 0.9902, val loss 1.6339\n",
      "step 36800: train loss 0.9952, val loss 1.6324\n",
      "step 36900: train loss 0.9906, val loss 1.6384\n",
      "step 37000: train loss 0.9892, val loss 1.6380\n",
      "step 37100: train loss 0.9918, val loss 1.6391\n",
      "step 37200: train loss 0.9928, val loss 1.6305\n",
      "step 37300: train loss 0.9924, val loss 1.6406\n",
      "step 37400: train loss 0.9893, val loss 1.6413\n",
      "step 37500: train loss 0.9907, val loss 1.6365\n",
      "step 37600: train loss 0.9900, val loss 1.6297\n",
      "step 37700: train loss 0.9866, val loss 1.6396\n",
      "step 37800: train loss 0.9854, val loss 1.6403\n",
      "step 37900: train loss 0.9872, val loss 1.6425\n",
      "step 38000: train loss 0.9872, val loss 1.6292\n",
      "step 38100: train loss 0.9860, val loss 1.6368\n",
      "step 38200: train loss 0.9835, val loss 1.6378\n",
      "step 38300: train loss 0.9872, val loss 1.6380\n",
      "step 38400: train loss 0.9851, val loss 1.6419\n",
      "step 38500: train loss 0.9833, val loss 1.6337\n",
      "step 38600: train loss 0.9811, val loss 1.6388\n",
      "step 38700: train loss 0.9780, val loss 1.6485\n",
      "step 38800: train loss 0.9815, val loss 1.6475\n",
      "step 38900: train loss 0.9816, val loss 1.6380\n",
      "step 39000: train loss 0.9767, val loss 1.6411\n",
      "step 39100: train loss 0.9824, val loss 1.6481\n",
      "step 39200: train loss 0.9757, val loss 1.6399\n",
      "step 39300: train loss 0.9804, val loss 1.6418\n",
      "step 39400: train loss 0.9786, val loss 1.6487\n",
      "step 39500: train loss 0.9765, val loss 1.6523\n",
      "step 39600: train loss 0.9737, val loss 1.6522\n",
      "step 39700: train loss 0.9726, val loss 1.6532\n",
      "step 39800: train loss 0.9709, val loss 1.6585\n",
      "step 39900: train loss 0.9734, val loss 1.6490\n",
      "step 40000: train loss 0.9692, val loss 1.6510\n",
      "step 40100: train loss 0.9675, val loss 1.6608\n",
      "step 40200: train loss 0.9693, val loss 1.6609\n",
      "step 40300: train loss 0.9677, val loss 1.6572\n",
      "step 40400: train loss 0.9681, val loss 1.6608\n",
      "step 40500: train loss 0.9683, val loss 1.6504\n",
      "step 40600: train loss 0.9672, val loss 1.6689\n",
      "step 40700: train loss 0.9646, val loss 1.6607\n",
      "step 40800: train loss 0.9644, val loss 1.6477\n",
      "step 40900: train loss 0.9655, val loss 1.6562\n",
      "step 41000: train loss 0.9648, val loss 1.6605\n",
      "step 41100: train loss 0.9637, val loss 1.6613\n",
      "step 41200: train loss 0.9658, val loss 1.6597\n",
      "step 41300: train loss 0.9595, val loss 1.6667\n",
      "step 41400: train loss 0.9649, val loss 1.6630\n",
      "step 41500: train loss 0.9620, val loss 1.6618\n",
      "step 41600: train loss 0.9598, val loss 1.6677\n",
      "step 41700: train loss 0.9584, val loss 1.6755\n",
      "step 41800: train loss 0.9609, val loss 1.6654\n",
      "step 41900: train loss 0.9633, val loss 1.6727\n",
      "step 42000: train loss 0.9601, val loss 1.6672\n",
      "step 42100: train loss 0.9574, val loss 1.6663\n",
      "step 42200: train loss 0.9557, val loss 1.6529\n",
      "step 42300: train loss 0.9542, val loss 1.6582\n",
      "step 42400: train loss 0.9582, val loss 1.6694\n",
      "step 42500: train loss 0.9556, val loss 1.6604\n",
      "step 42600: train loss 0.9529, val loss 1.6784\n",
      "step 42700: train loss 0.9587, val loss 1.6616\n",
      "step 42800: train loss 0.9527, val loss 1.6711\n",
      "step 42900: train loss 0.9530, val loss 1.6640\n",
      "step 43000: train loss 0.9488, val loss 1.6682\n",
      "step 43100: train loss 0.9500, val loss 1.6725\n",
      "step 43200: train loss 0.9503, val loss 1.6668\n",
      "step 43300: train loss 0.9541, val loss 1.6673\n",
      "step 43400: train loss 0.9503, val loss 1.6848\n",
      "step 43500: train loss 0.9525, val loss 1.6817\n",
      "step 43600: train loss 0.9482, val loss 1.6849\n",
      "step 43700: train loss 0.9496, val loss 1.6711\n",
      "step 43800: train loss 0.9482, val loss 1.6760\n",
      "step 43900: train loss 0.9474, val loss 1.6752\n",
      "step 44000: train loss 0.9455, val loss 1.6718\n",
      "step 44100: train loss 0.9468, val loss 1.6828\n",
      "step 44200: train loss 0.9424, val loss 1.6763\n",
      "step 44300: train loss 0.9422, val loss 1.6848\n",
      "step 44400: train loss 0.9456, val loss 1.6791\n",
      "step 44500: train loss 0.9395, val loss 1.6689\n",
      "step 44600: train loss 0.9424, val loss 1.6811\n",
      "step 44700: train loss 0.9390, val loss 1.6987\n",
      "step 44800: train loss 0.9413, val loss 1.6869\n",
      "step 44900: train loss 0.9384, val loss 1.6832\n",
      "step 45000: train loss 0.9372, val loss 1.7021\n",
      "step 45100: train loss 0.9396, val loss 1.6896\n",
      "step 45200: train loss 0.9397, val loss 1.6770\n",
      "step 45300: train loss 0.9385, val loss 1.6783\n",
      "step 45400: train loss 0.9348, val loss 1.6857\n",
      "step 45500: train loss 0.9390, val loss 1.6731\n",
      "step 45600: train loss 0.9348, val loss 1.6881\n",
      "step 45700: train loss 0.9366, val loss 1.6923\n",
      "step 45800: train loss 0.9397, val loss 1.6891\n",
      "step 45900: train loss 0.9321, val loss 1.6871\n",
      "step 46000: train loss 0.9311, val loss 1.6894\n",
      "step 46100: train loss 0.9326, val loss 1.6898\n",
      "step 46200: train loss 0.9313, val loss 1.7015\n",
      "step 46300: train loss 0.9295, val loss 1.6887\n",
      "step 46400: train loss 0.9299, val loss 1.6879\n",
      "step 46500: train loss 0.9302, val loss 1.6886\n",
      "step 46600: train loss 0.9293, val loss 1.6939\n",
      "step 46700: train loss 0.9308, val loss 1.6951\n",
      "step 46800: train loss 0.9291, val loss 1.6925\n",
      "step 46900: train loss 0.9250, val loss 1.7015\n",
      "step 47000: train loss 0.9269, val loss 1.7124\n",
      "step 47100: train loss 0.9280, val loss 1.7036\n",
      "step 47200: train loss 0.9241, val loss 1.7118\n",
      "step 47300: train loss 0.9255, val loss 1.7047\n",
      "step 47400: train loss 0.9269, val loss 1.7026\n",
      "step 47500: train loss 0.9239, val loss 1.7148\n",
      "step 47600: train loss 0.9211, val loss 1.7091\n",
      "step 47700: train loss 0.9223, val loss 1.7133\n",
      "step 47800: train loss 0.9245, val loss 1.7053\n",
      "step 47900: train loss 0.9208, val loss 1.7025\n",
      "step 48000: train loss 0.9200, val loss 1.7117\n",
      "step 48100: train loss 0.9212, val loss 1.7037\n",
      "step 48200: train loss 0.9206, val loss 1.7094\n",
      "step 48300: train loss 0.9207, val loss 1.7035\n",
      "step 48400: train loss 0.9181, val loss 1.7079\n",
      "step 48500: train loss 0.9158, val loss 1.7115\n",
      "step 48600: train loss 0.9175, val loss 1.7136\n",
      "step 48700: train loss 0.9143, val loss 1.7006\n",
      "step 48800: train loss 0.9202, val loss 1.7090\n",
      "step 48900: train loss 0.9174, val loss 1.7061\n",
      "step 49000: train loss 0.9135, val loss 1.7127\n",
      "step 49100: train loss 0.9145, val loss 1.7117\n",
      "step 49200: train loss 0.9096, val loss 1.7236\n",
      "step 49300: train loss 0.9107, val loss 1.7126\n",
      "step 49400: train loss 0.9125, val loss 1.7187\n",
      "step 49500: train loss 0.9142, val loss 1.7068\n",
      "step 49600: train loss 0.9155, val loss 1.7142\n",
      "step 49700: train loss 0.9125, val loss 1.7094\n",
      "step 49800: train loss 0.9172, val loss 1.7180\n",
      "step 49900: train loss 0.9116, val loss 1.7192\n",
      "step 50000: train loss 0.9119, val loss 1.7256\n",
      "step 50100: train loss 0.9104, val loss 1.7152\n",
      "step 50200: train loss 0.9060, val loss 1.7217\n",
      "step 50300: train loss 0.9080, val loss 1.7284\n",
      "step 50400: train loss 0.9022, val loss 1.7191\n",
      "step 50500: train loss 0.9088, val loss 1.7322\n",
      "step 50600: train loss 0.9057, val loss 1.7381\n",
      "step 50700: train loss 0.9062, val loss 1.7247\n",
      "step 50800: train loss 0.9059, val loss 1.7278\n",
      "step 50900: train loss 0.9071, val loss 1.7221\n",
      "step 51000: train loss 0.9058, val loss 1.7460\n",
      "step 51100: train loss 0.9063, val loss 1.7262\n",
      "step 51200: train loss 0.9018, val loss 1.7358\n",
      "step 51300: train loss 0.9041, val loss 1.7292\n",
      "step 51400: train loss 0.9036, val loss 1.7299\n",
      "step 51500: train loss 0.9028, val loss 1.7300\n",
      "step 51600: train loss 0.8981, val loss 1.7273\n",
      "step 51700: train loss 0.9034, val loss 1.7279\n",
      "step 51800: train loss 0.9014, val loss 1.7363\n",
      "step 51900: train loss 0.8999, val loss 1.7365\n",
      "step 52000: train loss 0.8982, val loss 1.7166\n",
      "step 52100: train loss 0.8989, val loss 1.7376\n",
      "step 52200: train loss 0.8980, val loss 1.7290\n",
      "step 52300: train loss 0.8971, val loss 1.7279\n",
      "step 52400: train loss 0.8995, val loss 1.7358\n",
      "step 52500: train loss 0.8954, val loss 1.7294\n",
      "step 52600: train loss 0.8959, val loss 1.7530\n",
      "step 52700: train loss 0.8978, val loss 1.7341\n",
      "step 52800: train loss 0.8936, val loss 1.7404\n",
      "step 52900: train loss 0.8969, val loss 1.7372\n",
      "step 53000: train loss 0.8956, val loss 1.7427\n",
      "step 53100: train loss 0.8942, val loss 1.7268\n",
      "step 53200: train loss 0.8947, val loss 1.7257\n",
      "step 53300: train loss 0.8903, val loss 1.7531\n",
      "step 53400: train loss 0.8910, val loss 1.7413\n",
      "step 53500: train loss 0.8886, val loss 1.7472\n",
      "step 53600: train loss 0.8920, val loss 1.7524\n",
      "step 53700: train loss 0.8893, val loss 1.7409\n",
      "step 53800: train loss 0.8897, val loss 1.7332\n",
      "step 53900: train loss 0.8882, val loss 1.7368\n",
      "step 54000: train loss 0.8886, val loss 1.7407\n",
      "step 54100: train loss 0.8851, val loss 1.7470\n",
      "step 54200: train loss 0.8852, val loss 1.7475\n",
      "step 54300: train loss 0.8897, val loss 1.7400\n",
      "step 54400: train loss 0.8904, val loss 1.7427\n",
      "step 54500: train loss 0.8842, val loss 1.7453\n",
      "step 54600: train loss 0.8879, val loss 1.7442\n",
      "step 54700: train loss 0.8898, val loss 1.7374\n",
      "step 54800: train loss 0.8839, val loss 1.7527\n",
      "step 54900: train loss 0.8872, val loss 1.7516\n",
      "step 55000: train loss 0.8862, val loss 1.7369\n",
      "step 55100: train loss 0.8837, val loss 1.7457\n",
      "step 55200: train loss 0.8848, val loss 1.7442\n",
      "step 55300: train loss 0.8807, val loss 1.7382\n",
      "step 55400: train loss 0.8843, val loss 1.7502\n",
      "step 55500: train loss 0.8784, val loss 1.7631\n",
      "step 55600: train loss 0.8782, val loss 1.7584\n",
      "step 55700: train loss 0.8780, val loss 1.7679\n",
      "step 55800: train loss 0.8800, val loss 1.7340\n",
      "step 55900: train loss 0.8780, val loss 1.7483\n",
      "step 56000: train loss 0.8809, val loss 1.7624\n",
      "step 56100: train loss 0.8769, val loss 1.7602\n",
      "step 56200: train loss 0.8765, val loss 1.7594\n",
      "step 56300: train loss 0.8768, val loss 1.7461\n",
      "step 56400: train loss 0.8767, val loss 1.7525\n",
      "step 56500: train loss 0.8796, val loss 1.7541\n",
      "step 56600: train loss 0.8764, val loss 1.7446\n",
      "step 56700: train loss 0.8753, val loss 1.7640\n",
      "step 56800: train loss 0.8706, val loss 1.7570\n",
      "step 56900: train loss 0.8702, val loss 1.7540\n",
      "step 57000: train loss 0.8752, val loss 1.7727\n",
      "step 57100: train loss 0.8724, val loss 1.7627\n",
      "step 57200: train loss 0.8743, val loss 1.7541\n",
      "step 57300: train loss 0.8787, val loss 1.7469\n",
      "step 57400: train loss 0.8696, val loss 1.7541\n",
      "step 57500: train loss 0.8710, val loss 1.7501\n",
      "step 57600: train loss 0.8723, val loss 1.7583\n",
      "step 57700: train loss 0.8742, val loss 1.7519\n",
      "step 57800: train loss 0.8715, val loss 1.7475\n",
      "step 57900: train loss 0.8690, val loss 1.7714\n",
      "step 58000: train loss 0.8690, val loss 1.7655\n",
      "step 58100: train loss 0.8651, val loss 1.7655\n",
      "step 58200: train loss 0.8686, val loss 1.7577\n",
      "step 58300: train loss 0.8661, val loss 1.7626\n",
      "step 58400: train loss 0.8641, val loss 1.7766\n",
      "step 58500: train loss 0.8634, val loss 1.7701\n",
      "step 58600: train loss 0.8674, val loss 1.7555\n",
      "step 58700: train loss 0.8661, val loss 1.7658\n",
      "step 58800: train loss 0.8648, val loss 1.7717\n",
      "step 58900: train loss 0.8642, val loss 1.7610\n",
      "step 59000: train loss 0.8642, val loss 1.7759\n",
      "step 59100: train loss 0.8590, val loss 1.7654\n",
      "step 59200: train loss 0.8616, val loss 1.7734\n",
      "step 59300: train loss 0.8600, val loss 1.7774\n",
      "step 59400: train loss 0.8626, val loss 1.7775\n",
      "step 59500: train loss 0.8623, val loss 1.7799\n",
      "step 59600: train loss 0.8596, val loss 1.7674\n",
      "step 59700: train loss 0.8615, val loss 1.7687\n",
      "step 59800: train loss 0.8611, val loss 1.7785\n",
      "step 59900: train loss 0.8605, val loss 1.7756\n",
      "step 59999: train loss 0.8562, val loss 1.7648\n"
     ]
    }
   ],
   "source": [
    "# Step 16: Run the pre-training loop\n",
    "\n",
    "# move model to device\n",
    "m = model.to(device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d5986",
   "metadata": {
    "id": "6Xr-Uk_6-QsX",
    "papermill": {
     "duration": 0.033015,
     "end_time": "2025-10-20T19:22:36.382556",
     "exception": false,
     "start_time": "2025-10-20T19:22:36.349541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Infrance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b930a44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T19:22:36.453299Z",
     "iopub.status.busy": "2025-10-20T19:22:36.452442Z",
     "iopub.status.idle": "2025-10-20T19:24:27.695278Z",
     "shell.execute_reply": "2025-10-20T19:24:27.694401Z"
    },
    "id": "ZVOn42m3-JBZ",
    "outputId": "86d003e1-6e63-450f-bcaa-b411aea49b07",
    "papermill": {
     "duration": 111.311808,
     "end_time": "2025-10-20T19:24:27.728470",
     "exception": false,
     "start_time": "2025-10-20T19:22:36.416662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "First Murderer:\n",
      "Were the great toe! how would they are to return,\n",
      "As the people gentlemen, make Henry breath\n",
      "The Lambond of itch virtue gates,\n",
      "The land another before the time when I have intended\n",
      "As these shrug, the father and the time\n",
      "Of the several well-insportuned love?\n",
      "\n",
      "CATESBY:\n",
      "My father's graves the heaven, mark not the proof,\n",
      "And the senators could encounter of mine:\n",
      "The offices of acceptance and age so bondly.\n",
      "\n",
      "LUCIO:\n",
      "Why, Belteness love, or your heart is now full alies\n",
      "acred mouths our servant. What\n",
      "could yet that yet think not how to die, my lord,\n",
      "Against my secret blood things entreaty.\n",
      "Scorn me not, good my lord; be little world:xt them\n",
      "both this disO, call a feature of death; babes\n",
      "To sword by the action of his months much.\n",
      "\n",
      "ISABELLA:\n",
      "Gentle my life\n",
      "By correction onion, I'll be the sun that I stand:\n",
      "Rich is it required to be other side of sharp,\n",
      "Go back, sick, under Oxford,\n",
      "Is it ten to year, when you shall happen the heavy\n",
      "Beggartic cannot tent a courtier; go\n",
      "To dear quick at liberty.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Thy love to stand and to bed, to answer his\n",
      "thief: very well the devil's censure.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "Hold it not cleft the stone, treasons of the one,\n",
      "Or I would be guilto cannot speak that mock him,\n",
      "She would say to hear sad some more home than me?\n",
      "\n",
      "ANTIGONUS:\n",
      "Go to, go to: and these plain billers alive\n",
      "That love bids me so faith and change to repent,\n",
      "Lest 'em up indeed I take my leave.\n",
      "So much better my letters-time dry,\n",
      "That were long-keeper at her my head.\n",
      "The king speak it af the great for encounter?\n",
      "\n",
      "BUCKINGHAM:\n",
      "No; I know not when your deed love action\n",
      "To be of France, or as those that feed us\n",
      "With the heart that humble at liberty.\n",
      "\n",
      "GREEN:\n",
      "God and said Hereford and thus him have?\n",
      "\n",
      "AUTOLYCUS:\n",
      "Enought the new shoemakers shall be the silver\n",
      "Than that some shall be leve,--if Warwick be thou slay'st;\n",
      "For miseries have me, man? where rejoicifies\n",
      "Do you convey my cushion, I must have made\n",
      "You sport it again to make our crown,\n",
      "How he hath wearst to choo\n"
     ]
    }
   ],
   "source": [
    "# Step 17: Inference\n",
    "\n",
    "# generate from the model. Not great. Not too bad either\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20d885",
   "metadata": {
    "id": "lSFw5TEs_S1I",
    "papermill": {
     "duration": 0.033972,
     "end_time": "2025-10-20T19:24:27.796116",
     "exception": false,
     "start_time": "2025-10-20T19:24:27.762144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 🧪 Notes & Recommendations\n",
    "\n",
    "This project is built for **experimentation and research demonstration**.  \n",
    "If you want **higher accuracy or more stable results**, try the following:\n",
    "\n",
    "- 🔁 **Increase training iterations** — Run `max_iters` between **60K–100K** if you have a **high-end GPU (RTX 4090 / A100 / H100)**.  \n",
    "- ⚙️ **Tune hyperparameters** — Experiment with `learning_rate`, `num_experts`, `top_k`, and `n_embed` for your dataset scale.\n",
    "- 💾 **Save and share your trained weights** — Push the best-performing checkpoints to **[Hugging Face Hub](https://huggingface.co)** for community use.\n",
    "- 💻 **Deploy interactively** — Create a **Gradio** or **Streamlit** web interface to chat or generate text directly from the model.\n",
    "\n",
    "---\n",
    "\n",
    "✅ *This notebook is a complete end-to-end implementation — from dataset to inference.*  \n",
    "💡 *If you enjoyed this project or found it useful, please consider giving it an ⭐ on GitHub or an upvote on Kaggle!*\n",
    "\n",
    "**Thank you!**\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36647.067334,
   "end_time": "2025-10-20T19:24:29.453893",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-20T09:13:42.386559",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
